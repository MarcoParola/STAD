\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{frontespizio}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{scrextend}
\usepackage[margin=1.2in]{geometry}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{url}

\begin{document}
\selectlanguage{english}
\baselineskip 13pt
	
% ---- FRONTESPIZIO ----- 
\begin{frontespizio} 
 \Preambolo{\renewcommand{\frontpretitlefont}{\fontsize{15}{12}\scshape}}
\Istituzione {University of Pisa}
\Divisione {Scuola di Ingegneria}
\Corso [Laurea]{Artificial Intelligence and Data Engineering}
\Annoaccademico {2019--2020}
\Titolo { \vspace {35mm}Documentation of the\\"STAD" Application}
\Filigrana [height=4cm,before=0.28,after=1]{./images/stemma_unipi.png}
\Rientro {1cm}
\Candidato {Alice Nannini}
\Candidato {Marco Parola}
\Relatore {Prof. Francesco Marcelloni}
\Relatore {Prof. Pietro Ducange}
 \Punteggiatura {}
\end{frontespizio}


% ----- INDICE -----
\tableofcontents\thispagestyle{empty}
\clearpage


\section{Introduction}\pagenumbering{arabic}
The goal of this application is to prevent and detect situations potentially dangerous, caused by huge amounts of rain, scraping twitter and analyzing each tweet, in order to discover some tweets containing information related to these critical situations.\\
We analyze the data and develop the application using Python and Sklearn library.

\section{The Data}
\subsection{Retrieve the data}
The data, on which this application works, are tweets. In order to collect enought tweets, we scraped twitter, using \textbf{twint}.\\
Twint is an advanced opensource Twitter scraping tool, written in Python, thanks to which it's very easy to collect data, according to some criteria, and store them in csv files.
For more information about \textit{twintproject} visit the Github repository: \url{https://github.com/twintproject}.
\begin{verbatim}
   twint -s <WORDS> -o tweetPioggia.csv --csv

   <WORDS> : 'pioggia', 'piove', 'allerta', 'meteo', 'alluvione', 'maltempo'
\end{verbatim}
Examples:
\begin{itemize}
\item "Ora piove a dirotto per la gioia di yuki che non può andare al parco"
\item "Ma dai, ma piove sul bagnato! Povera Antonella!!!!! \#GFVIP"
\end{itemize}
Moreover we added to the dataset some posts randomly downloaded, not related to any weather phenomenons (without specifying any keywords).

\subsection{Prepare the dataset}
After collectioning the tweets (894), we assigned each of them to a class, in order to prepare the dataset, thanks to which we can build some classifiers.\\
We decided to map tweets in 3 classes:
\begin{itemize}
\item 0 -> the tweet is not related to a weather condition (368)
\item 1 -> the tweet is about rain or some weather condition not dangerous (224)
\item 2 -> the tweet is about some dangerous situation caused by the rain (302)
\end{itemize}

\section{Preprocessing}
In this phase, we delete some tweets, in order to manage only the italian tweets.\\
Moreover we clean the text of each tweet removing eventual URLs.

\section{Text Elaboration}
After preprocessing phase we follow the standard steps to manage text:
\begin{itemize}
\item tokenization and stemming
\item words selection by tf-idf
\item classification and evaluation
\end{itemize}

\subsection{TOKENIZATION AND STEMMING}
    After stratified splitting the dataset in tranining and test set, we tokenize each tweet and we apply an italian stemming filter, in order to find more general words.
    
        italian_stemmer = SnowballStemmer('italian')
        class StemmedCountVectorizer(CountVectorizer):
            def build_analyzer(self):
                analyzer = super(StemmedCountVectorizer, self).build_analyzer()
                return lambda doc: ([italian_stemmer.stem(w) for w in analyzer(doc)])

                
                
2_ FIND RELEVANT STEMS
    All the stems are united in one vector and are weighted using the IDF index (Inverse Document Frequency)
    Then, each training tweet is represented as a vector of features, dimension equal to the number of stems, and the i-th feature is
    calculated as the frequency of the i-th stem in the tweet per the weight of that stem.

    Finally each stem is evaluated by the Information Gain (IG) value between the corresponding feature and the possible class labels:
        IG(C, Sq) = H(C) − H(C|Sq)
    where Sq is the feature corresponding to stem sq, H(C) is the entropy of C, and H(C|Sq ) is the entropy of C after the observation 
    of the feature Sq.
    Then, the stems are ranked in descending order and F stems, with F ≤ Q, are selected among these.
    
        tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)# include calculation of TFs (frequencies) 
        X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
        
        
3_ CLASSIFICATION
    After the tfidf_transformer fitting phase we use it to trasnform the test set and we build and test different classifiers and compute different metrics to compare them (accurancy, f-score, confusion-matrix):
    
    - DecisionTreeClassifier
        
        < SCREEN >

    - GaussianNB
        
        < SCREEN >

    - SVC 
        
        < SCREEN >

    - RandomForestClassifier
    
        < SCREEN >

    - AdaBoostClassifier
        
        < SCREEN >

    - KNeighborsClassifier
        
        < SCREEN >
        







\end{document}