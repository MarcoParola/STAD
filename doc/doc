--------
| DATA |
--------

 - RETRIEVE DATA
The data, on which this application works, are tweets. In order to collect enought tweets, we scraped twitter, using twint.
Twint is an advanced opensource Twitter scraping tool, written in python, thanks which it is very easy collect data, according to some criteria and store them in csv files.
For more informations about twintproject visit the Github repository: https://github.com/twintproject .

 - PREPARE DATA-SET
After collectiong tweet, we assign each of them to a class, in order to prepare the dataset, thanks to which we can build some classifiers.
We decided to map tweets in 3 classes:
0 -> the tweet is not related to a weather condition
1 -> the tweet is about rain or some weathet condition not dangerous
2 -> the tweet is about some dangerous situation caused by the rain
 
 
 
-----------------
| PREPROCESSING |
-----------------

A tweet is processed in order to be transormed in a vector of number.
The prepocessing phase is compute using python, in particular nltk package (Natural Language ToolKit) and sklearn package (scikit-learn).

1_ TOKENIZATION (nltk)
    Split each words into a array
    
2_ APPLY A SYOP-WORLD FILTER (nltk)
    This filter remove all conjunctions, preposition, articles, etc. This phase is strongly dependent on the language (italian in our case).

3_ STEMMING (nltk)
    Reduce the similar words to a common root

4_ VECTORIZATION (sklearn)
    Map the result of stemming phase, in a predefined vector counting the frequences of some prefixed keywords
    


-----------------------------
| SUPERVISED LEARNING STAGE |
-----------------------------

1_ FIND RELEVANT STEMS
	Build the training set, searching tweets based on specific context-related words.
	// Employ a collection of labeled tweets as training set.
	Each tweet is processed through a tokenization phase, a stop-word filter and a stemming phase.
	All the stems are united in one vector and are weighted using the IDF index (Inverse Document Frequency)
	Then, each training tweet is represented as a vector of features, dimension equal to the number of stems, and the i-th feature is
	calculated as the frequency of the i-th stem in the tweet per the weight of that stem.
	
	Finally each stem is evaluated by the Information Gain (IG) value between the corresponding feature and the possible class labels:
		IG(C, Sq) = H(C) − H(C|Sq)
	where Sq is the feature corresponding to stem sq, H(C) is the entropy of C, and H(C|Sq ) is the entropy of C after the observation 
	of the feature Sq.
	Then, the stems are ranked in descending order and F stems, with F ≤ Q, are selected among these.


    
----------------------
| CLASSIFIER MODELLS |
----------------------

In this phase we built different classifiers, belong to sklearn package and we compared them.

- DecisionTreeClassifier
Simple to understand and to interpret.

    < SCREEN >

- GaussianNB
Gaussian Naive Bayes can perform online updates to model parameters via partial_fit method.

- SVC 
Complexity O(n^2) where n is the number of samples, it is impractical beyond tens of thousands of samples.

- RandomForestClassifier
Random forests or random decision forests are an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes.

- AdaBoostClassifier
An AdaBoost is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.

- KNeighborsClassifier
Classifier implementing the k-nearest neighbors vote. It is important find the best value of k.
